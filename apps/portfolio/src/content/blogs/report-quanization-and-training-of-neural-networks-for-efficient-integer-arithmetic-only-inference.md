---
title: 論文読み - Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference
description: Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference という量子化の論文を読みました。
publishedAt: 2024/07/03
---

今回は "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference"[^1]という論文を読みました。

[^1]: [Shibani Santurkar, Dimitris Tsipras, Brandon Tran, Andrew Ilyas, Logan Engstrom, Aleksander Madry, "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference", 2018](https://arxiv.org/abs/1712.05877)

## 導入

近年のDLベースのモデルは膨大な計算コストがかかることから、エッジデバイスやモバイルデバイスでの実行が難しいという問題があり、これを解決するために既存のモデルを新たな手法により**量子化**することを提案している。

近年のモバイルプラットフォーム(== スマートフォン, AR/VR, ドローンなど)でのDLモデルでは、**モデルサイズ**と**推論時間**を削減することによって実行を可能にしその分野での研究が盛んであるが、実際にハードウェアに対して検証しているわけではなく、あまり有効ではない高速化（ビットシフトやバイナリネットワークなど）が多い。
実際に比べるべくは**精度**と**ハードウェア上での推論速度**のトレードオフである。

## Quantization Scheme

論文では新たな量子化手法として以下の方法を提案している。

$$
r = S(q - Z)
$$

- $q$ : 量子化された値 (integer)
- $Z$ : 量子化された値の中心 (offset) の値 (integer)
- $S$ : 量子化された値のスケールの値 (float)
- $r$ : 量子化された値の実数値 (float)

$B$-bit 量子化に対して、$q$はB-bitの符号付き整数で表現される。

この時、パラメータとなるのは定数$Z$と$S$である。

これを行列演算に適用することを考える。

量子化された$N \times N$行列$r_1, r_2$に対して、$r_3 = r_1 \cdot r_2$を計算するとする。

行列$r_{\alpha}$の要素$r_{\alpha}^{(i, j)}$は以下のように表現される。

$$
r_{\alpha}^{(i, j)} = S_{\alpha} (q_{\alpha}^{(i, j)} - Z_{\alpha})
$$

そして行列積の結果$r_{3}^{(i, j)}$は以下のように表現される。

$$
r_{3}^{(i, j)} = \sum_{k=1}^{N} r_{1}^{(i, k)} \cdot r_{2}^{(k, j)}
$$

これを用いて、行列演算を$q_{\alpha}$に対して表現すると以下のようになる。

$$
\begin{aligned}
S_{3} (q_{3}^{(i, j)} - Z_{3}) &= \sum_{k=1}^{N} S_{1} (q_{1}^{(i, k)} - Z_{1}) \cdot S_{2} (q_{2}^{(k, j)} - Z_{2}) \\
S_{3} (q_{3}^{(i, j)} - Z_{3}) &= S_{1} S_{2} \sum_{k=1}^{N} (q_{1}^{(i, k)} - Z_{1}) \cdot (q_{2}^{(k, j)} - Z_{2}) \\
q_{3}^{(i, j)} - Z_{3} &= \frac{S_{1} S_{2}}{S_{3}} \sum_{k=1}^{N} (q_{1}^{(i, k)} - Z_{1}) \cdot (q_{2}^{(k, j)} - Z_{2}) \\
q_{3}^{(i, j)} &= Z_{3} + M \sum_{k=1}^{N} (q_{1}^{(i, k)} - Z_{1}) \cdot (q_{2}^{(k, j)} - Z_{2})
\end{aligned}
$$

この時、$M = \frac{S_{1} S_{2}}{S_{3}}$で、この式において唯一の非負整数パラメータである。

求めた数式より、量子化結果は$Z_{3}$と$M$に依存し、これはどちらも定数であるため、オフライン（not 学習時）で計算することができる。

しかし、$M$がfloatゆえに結局目的の整数演算のみでの推論は達成できない。
論文では、経験的に$M$が$[0, 1)$の範囲に収まることから、次のように$M$を表現することを提案している。

$$
M = 2^{-n} M_0
$$

$M_0$は$[0.5, 1)$の範囲に収まるように制約をかけ、その中でハードウェアの制約に適した$n$を使うことで固定小数点乗数を用いた整数演算のみでの推論を実現する。

実際にどれくらいの計算量で量子化が可能かどうかを考えると、$q_3^{{(i, j)}}$を展開して因数分解したときに

$$
\begin{aligned}
q_3^{{(i, j)}} &= Z_3 + M \sum_{k=1}^{N} (q_1^{{(i, k)}} - Z_1) \cdot (q_2^{{(k, j)}} - Z_2) \\
&= Z_3 + M \left( N Z_1 Z_2 - Z_1 \sum_{k=1}^{N} q_2^{{(k, j)}} - Z_2 \sum_{k=1}^{N} q_1^{{(i, k)}} + \sum_{k=1}^{N} q_1^{{(i, k)}} q_2^{{(k, j)}} \right)
\end{aligned}
$$

となり、支配的な部分の計算量はせいぜい$O(N^3)$であることがわかる。

一つのパーセプトロンの値を計算するのに必要なデータ量を考える。
論文図1.1 (a)のように、入力と重みはそれぞれ`uint8`で量子化されているものである時、$y = \sum_{i=1}^{N} x_i w_i$を計算することを考えると`uint8^2`の和なので、`uint32`として`y`の値を持つことができる。
その時、バイアスは`y`にそのまま加算されるため同じ次元である`uint32`で量子化された値を持つこととなるが、モデルサイズに対してバイアスがそこまで大きな割合を占めておらず、バイアスは重みに比べ全体に作用するために精度が下がりやすいため、`uint32`で持つのだと論文では述べられている。

そしてoutputはinputと同じ次元でないといけないため`uint32`出力を`uint8`にダウンキャストしてから活性化関数に通してoutputを得る。

## Training

一般的な量子化のアプローチでは、表現空間の大きいパラメータで学習された重みを、学習後に小さい表現空間に量子化するというものだった。
論文では**Post-training quantization**と称されているこちらのアプローチは、大規模モデルでは機能するが**小規模モデルでは精度が著しく低下する**ことが知られている。
当たり前だが、解像度が等しくなるように量子化されるため、もし**近い値が偏って分布していた場合**にそれが量子化後に**同じ値として丸められてしまう**ことがあり、小さなモデルであればあるほどその影響が大きくなるのである。

そのため、論文ではうまく学習時に重みを**量子化前提で調整**させることを提唱している。
実際には、学習時には重みを`float32`で保持し、Backpropagationもそのまま行うが、モデルがforward(推論)する際の**全結合層のアウトプットが活性化関数を通った後**などのタイミングで実際の値ではなく量子化された値を扱うことで擬似的に学習時に量子化されることを想定した学習を行う。

この実際は勾配更新などは`float32`で行われているが、出力を量子化された値として学習するやり方を**Fake quantization**と呼ぶ。

$$
\begin{aligned}
\text{clamp}(r; a, b) &:= \min (\max (r, a), b) \\
s(a, b, n) &:= \frac{b - a}{n - 1} \\
q(r; a, b, n) &:= \left\lfloor \frac{\text{clamp}(r; a, b) - a}{s(a, b, n)} \right\rceil s(a, b, n) + a
\end{aligned}
$$

式を見ればわかるように実際は`float32`の値だが、`uint8`の値のスケールで丸める操作をしている。

> [!NOTE]
> $\lfloor x \rceil$は$x$を最も近い整数に丸める関数である。

モデルによってはBatch Normalizationがレイヤーに含まれている場合があるが、Fake quantizationを適用するにはBatch Normalizationによってスケールされた後の重みを量子化する必要があるため工夫が必要である。論文式(14)のようにEMA(折りたたみ結果の指数移動平均)を用いてBatch Normalizationのスケールを推定し、それに応じた量子化を行う。

## Experiments

### 大規模モデルにおける量子化

ResNetとInception-v3の2つのモデルを用いて、通常の学習と量子化学習をした後に量子化されたモデルを評価した。

ResNetを使ってさまざまなレイヤーの深さで学習した結果、Integer-quantizedなモデルはFloat-pointモデルに比べて最大でも2%程度の精度低下に抑えられている。

また、従来手法と比較したときに精度とモデルサイズのトレードオフをうまく調整できていることがわかり、また単純には比較できないが実行速度改善ができた。

次にInception-v3を使って、活性化量子化の精度を調べた。

1ビット減らした7bitsの量子化でも、8bitsと遜色ない性能が出ることがわかり、さらに精度はReLUよりもReLU6の方が1bit減らした時の精度劣化が少ないことがわかった。

ちなみに、ReLU6はReLUの出力を$[0, 6]$にクリップしたもので、ReLUより小さなチャネルに縮まった状態で学習されるため、量子化された時に精度が落ちにくいようだ。

### MobileNetとの比較

すでにImageNetの分類タスクをエッジデバイスで行うために推論コストと精度のトレードオフの最先端の成果を出しているMobileNetと、それを本手法で量子化したモデルを比較した。

論文図1.1(c),4.1,4.2にて、各チップでMobileNetのモデル(Float)に対して、今回の量子化モデル(8-bit)と本家MobileNetと同等の精度を出すのにかかる推論コスト(=推論時間)を比較している。

試したデバイスは

- Snapdragon 835 LITTLE
- Snapdragon 835 BIG
- Snapdragon 821

で、それぞれのデバイスでMobileNetのモデルに対して、本手法で量子化したモデルが同等の精度を出すのにかかる推論コストがどれくらいかを比較している。
835 LITTLEでは推論レイテンシが大体半分程度に削減できていることがわかった。（論文では対数スケールでレイテンシが表現されているので注意）
さらに、同じ推論時間で出せる精度という軸で見ると、10%程度の精度向上が見られた。
一方で821はそもそものチップが浮動小数点演算がより最適化されているため、それほど整数演算ベースに量子化したモデルが有利にならなかった。

COCOという物体検知評価のデータセットを使った比較では、論文図4.4のように実行時間を最大50%程度量子化により削減できており、チップの性能が良いことが量子化の効果をより引き出すことができることがわかった。

Face Detectionのタスクでも、MobileNetのモデルに対して本手法で量子化したモデルが同等の精度を出すのにかかる推論コストがどれくらいかを比較している。

結果約2%の精度低下と引き換えに2倍の推論速度向上が見られた。
さらに使うスレッド数を増やした時にうまく速度も向上したことからマルチスレッドに対しても有効であることがわかった。
（ただし元モデルの速度が比較されていないのは気になる）

- Precision
- Recall
- DM

### パラメータ比較

量子化の重みと活性化それぞれのbit数を変化させて精度低下を調べたところ、重みが活性化よりも精度低下に敏感であること、7,8ビットくらいであればそこまで性能低下を気にせず元モデルと同等の精度を出せること、総bit数が同じであれば重みと活性化のbit数を等しくするのが良いことがわかった。

## まとめ

量子化をシミュレートして学習させる手法により、精度低下を最小限に抑えつつ、推論速度を向上させることができることがわかった。
ARM NEONなどの整数演算ベースのハードウェアに対してとても有効であることがわかった。
GPUの力をそこまで使えないエッジデバイスにおいて、モバイルデバイスでの推論を実現するためには、このような量子化手法が有効であることがわかった。

